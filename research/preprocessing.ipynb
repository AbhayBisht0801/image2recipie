{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class  DataPreprocessConfig:\n",
    "    root_dir:Path\n",
    "    input_preprocess:Path\n",
    "    image_input:Path\n",
    "    text_input:Path\n",
    "    output_preprocess:Path\n",
    "    unzip_dir:Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bisht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from Image2Recipie.constants import *\n",
    "from Image2Recipie.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath=CONFIG_FILE_PATH,\n",
    "            params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config=read_yaml(config_filepath)\n",
    "        self.params=read_yaml(params_filepath)\n",
    "        create_directories([self.config.preprocessing.root_dir])\n",
    "\n",
    "    def get_data_preprocessing_config(self)->DataPreprocessConfig:\n",
    "        config=self.config\n",
    "        create_directories([config.preprocessing.root_dir])\n",
    "        create_directories([config.preprocessing.input_preprocess])\n",
    "        create_directories([config.preprocessing.output_preprocess])\n",
    "        data_preprocess_config=DataPreprocessConfig(\n",
    "            root_dir=config.preprocessing.root_dir,\n",
    "            input_preprocess=config.preprocessing.input_preprocess,\n",
    "            image_input=config.preprocessing.image_input,\n",
    "            text_input=config.preprocessing.text_input,\n",
    "            output_preprocess=config.preprocessing.output_preprocess,\n",
    "            unzip_dir=config.data_ingestion.unzip_dir\n",
    "        )\n",
    "        return data_preprocess_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request \n",
    "import gdown\n",
    "from Image2Recipie import logger\n",
    "import tensorflow as tf\n",
    "from Image2Recipie.utils.common import save_object,get_maxlen,clean_ingredients,CustomTokenizer,detect_english_text\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocess:\n",
    "    def __init__(self,config:DataPreprocessConfig):\n",
    "        self.config = config\n",
    "        self.input_image=None \n",
    "    \n",
    "        \n",
    "\n",
    "    def output_preprcess(self):\n",
    "        self.data=pd.read_csv(os.path.join(self.config.unzip_dir,'cuisine_updated.csv'))\n",
    "        self.features={}\n",
    "        self.data['cleaned_ingredient']=self.data['cleaned_ingredient'].apply(clean_ingredients)\n",
    "        self.data['cleaned_ingredient']=self.data['cleaned_ingredient'].apply(lambda x:','.join([x.strip() for x in x.split(',')]))\n",
    "        tokenizer=CustomTokenizer()\n",
    "        y_tokenized=tokenizer.fit_transform(self.data['cleaned_ingredient'])\n",
    "       \n",
    "        max_output=max(len(x) for x in y_tokenized)\n",
    "        ytoken=pad_sequences(y_tokenized,maxlen=max_output,padding='post')\n",
    "        y_output=to_categorical(ytoken,num_classes=None)\n",
    "        y_encoder_input=[]\n",
    "        for i in range(0,ytoken.shape[0]):\n",
    "            y_encoder_input.append((np.insert(ytoken[i][:-1],0,0)))\n",
    "        y_encoder=np.array(y_encoder_input)\n",
    "        \n",
    "        self.features['max_outputtoken']=max_output\n",
    "        self.features['output_vocab_size']=len(tokenizer.get_word_index())\n",
    "        save_object(os.path.join(self.config.input_preprocess,'y_input_encoder.pkl'),y_encoder)\n",
    "        save_object(os.path.join(self.config.output_preprocess,'y_tokenized.pkl'),y_output)\n",
    "        save_object(os.path.join(self.config.output_preprocess,'tokenizer.pkl'),tokenizer)\n",
    "        \n",
    "        logger.info(f'The Preprocessed output is saved in {self.config.output_preprocess}')\n",
    "    def input_preprocess(self):\n",
    "        non_english=detect_english_text(df=self.data)\n",
    "        non_english=detect_english_text(df=self.data)\n",
    "        self.data.drop(index=non_english,inplace=True)\n",
    "        self.data.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        maxlen=get_maxlen(self.data['name'])\n",
    "        token=Tokenizer()\n",
    "        token.fit_on_texts(self.data['name'])\n",
    "        \n",
    "        Xtoken=token.texts_to_sequences(self.data['name'])\n",
    "        Xtoken=pad_sequences(Xtoken,maxlen=maxlen)\n",
    "        self.features['input_maxlen']=maxlen\n",
    "        vocab_size=len(token.word_index)\n",
    "        self.features['input_vocal_size']=vocab_size\n",
    "        X=[]\n",
    "        for i in self.data['image_path']:\n",
    "            \n",
    "            img = cv.imread(os.path.join(self.config.unzip_dir, \"data/data/\" + i))\n",
    "            img=cv.resize(img,(160,160))\n",
    "            X.append(img)\n",
    "        input_image=np.array(X)\n",
    "        save_object(self.config.image_input,input_image)\n",
    "        save_object(self.config.text_input,Xtoken)\n",
    "        save_object(os.path.join(self.config.input_preprocess,'input_tokenizer.pkl'),token)\n",
    "        print(self.features)\n",
    "        save_object(os.path.join(self.config.root_dir,'features.pkl'),self.features)\n",
    "        logger.info(f'The Preprocessed input is saved in {self.config.input_preprocess}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-03 22:58:52,188:INFO:common:yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-11-03 22:58:52,207:INFO:common:yaml file: params.yaml loaded successfully]\n",
      "[2024-11-03 22:58:52,214:INFO:common:created directory at: artifacts/preprocessing]\n",
      "[2024-11-03 22:58:52,222:INFO:common:created directory at: artifacts/preprocessing]\n",
      "[2024-11-03 22:58:52,224:INFO:common:created directory at: artifacts/preprocessing/inputs_preprocess]\n",
      "[2024-11-03 22:58:52,225:INFO:common:created directory at: artifacts/preprocessing/output_preprocess]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-03 22:59:00,290:INFO:1171314784:The Preprocessed output is saved in artifacts/preprocessing/output_preprocess]\n",
      "{'max_outputtoken': 44, 'output_vocab_size': 992, 'input_maxlen': 80, 'input_vocal_size': 2391}\n",
      "[2024-11-03 22:59:52,914:INFO:1171314784:The Preprocessed input is saved in artifacts/preprocessing/inputs_preprocess]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config=ConfigurationManager()\n",
    "    data_preprocessing_config=config.get_data_preprocessing_config()\n",
    "    data_preprocessing=DataPreprocess(config=data_preprocessing_config)\n",
    "    \n",
    "    data_preprocessing.output_preprcess()\n",
    "    data_preprocessing.input_preprocess()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_ingredient\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misnull()]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[df['cleaned_ingredient'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
